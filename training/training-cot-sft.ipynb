{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Multi-GPU Training (No notebook_launcher)\n",
    "\n",
    "This is the **simplest** approach for multi-GPU training in Jupyter notebooks.\n",
    "\n",
    "## Key: Just DON'T use device_map!\n",
    "\n",
    "When you don't specify `device_map`, the Trainer will automatically:\n",
    "1. Detect all available GPUs\n",
    "2. Use DataParallel or DistributedDataParallel\n",
    "3. Distribute training across all GPUs\n",
    "\n",
    "That's it! No notebook_launcher needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Packages installed!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install packages\n",
    "!pip install -q transformers datasets peft accelerate trl wandb\n",
    "print(\"✅ Packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "2025-10-12 23:20:01.399705: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760311201.422059     278 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760311201.429261     278 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SYSTEM CHECK\n",
      "================================================================================\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "Number of GPUs: 2\n",
      "  GPU 0: Tesla T4\n",
      "  GPU 1: Tesla T4\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Imports and GPU check\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "import math\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SYSTEM CHECK\")\n",
    "print(\"=\"*80)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs: {num_gpus}\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Configuration\nimport os\n\nMODEL_NAME = \"rzeraat/qwen-2-0-5b-law-lora\"\nDATASET_NAME = \"rzeraat/law\"\nOUTPUT_DIR = \"./pactoria-v1-simple\"\n\n# API Keys - Load from environment variables for security\nWANDB_API_KEY = os.getenv('WANDB_API_KEY')\nHUGGINGFACE_API_KEY = os.getenv('HUGGINGFACE_TOKEN')  # Uses HUGGINGFACE_TOKEN from environment\n\n# W&B Configuration\nWANDB_PROJECT = \"uk-legal-training\"\nWANDB_ENABLED = True if WANDB_API_KEY else False\n\n# LoRA config\nLORA_R = 32\nLORA_ALPHA = 64\nLORA_DROPOUT = 0.05\nTARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n\n# Training config\nBATCH_SIZE = 5\nGRADIENT_ACCUMULATION_STEPS = 1\nLEARNING_RATE = 1e-3\nNUM_EPOCHS = 3\nMAX_SEQ_LENGTH = 700\n\n# W&B run name\nWANDB_RUN_NAME = f\"qwen-0.5b-law-r{LORA_R}-seq{MAX_SEQ_LENGTH}-bs{BATCH_SIZE}x{GRADIENT_ACCUMULATION_STEPS}-ep{NUM_EPOCHS}\"\n\nprint(f\"✅ Configuration loaded\")\nprint(f\"   Model: {MODEL_NAME}\")\nprint(f\"   Dataset: {DATASET_NAME}\")\nprint(f\"   Output: {OUTPUT_DIR}\")\nif WANDB_ENABLED:\n    print(f\"   W&B: {WANDB_PROJECT}/{WANDB_RUN_NAME}\")\nelse:\n    print(f\"   ⚠️  W&B disabled (WANDB_API_KEY not found in environment)\")"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "✅ Tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "print(\"✅ Tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "✅ Model loaded (params: 494,032,768)\n",
      "   Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load model - NO DEVICE_MAP!\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"sdpa\",\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "print(f\"✅ Model loaded (params: {model.num_parameters():,})\")\n",
    "print(f\"   Device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA...\n",
      "trainable params: 8,798,208 || all params: 502,830,976 || trainable%: 1.7497\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Apply LoRA\n",
    "print(\"Applying LoRA...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Load and format dataset with train/val/test splits - ENHANCED WITH SAMPLE TYPES\ndef format_sample(sample):\n    \"\"\"\n    Format a legal sample for training with support for multiple sample types.\n    \n    Sample Types:\n    - case_analysis: IRAC methodology (Issue → Rule → Application → Conclusion)\n    - educational: Structured teaching (Definition → Legal Basis → Key Elements → Examples)\n    - client_interaction: Practical advice (Understanding → Legal Position → Options → Recommendation)\n    - statutory_interpretation: Legislative analysis (Statutory Text → Purpose → Interpretation → Case Law)\n    \"\"\"\n    \n    # Get sample type (default to case_analysis for backward compatibility)\n    sample_type = sample.get('sample_type', 'case_analysis')\n    \n    # Build instruction (same for all types)\n    instruction = f\"\"\"### Instruction:\n{sample['question']}\n\n### Response:\"\"\"\n    \n    # Start response with metadata\n    response = f\"\"\"\n### Sample Type: {sample_type}\n### Topic: {sample.get('topic', 'Corporate Law')}\n### Difficulty: {sample.get('difficulty', 'intermediate')}\n\"\"\"\n    \n    # Add reasoning section (same for all types - chain of thought)\n    if 'reasoning' in sample and sample['reasoning']:\n        response += f\"\"\"\n### Reasoning:\n{sample['reasoning']}\n\"\"\"\n    \n    # Add answer section with type-specific formatting hints\n    # These hints help the model learn different answer structures\n    type_hints = {\n        'case_analysis': '(IRAC: Issue → Rule → Application → Conclusion)',\n        'educational': '(Teaching: Definition → Legal Basis → Key Elements → Examples)',\n        'client_interaction': '(Client Advice: Understanding → Position → Options → Recommendation)',\n        'statutory_interpretation': '(Statutory: Text → Purpose → Interpretation → Case Law)'\n    }\n    \n    hint = type_hints.get(sample_type, '')\n    \n    response += f\"\"\"\n### Answer {hint}:\n{sample['answer']}\"\"\"\n    \n    # Add case citations if available\n    if 'case_citation' in sample and sample['case_citation']:\n        response += f\"\"\"\n\n### Case Citation:\n{sample['case_citation']}\"\"\"\n    \n    return {\"text\": instruction + response}\n\nprint(\"Loading and formatting dataset with sample type support...\")\ndataset = load_dataset(DATASET_NAME)\nformatted_dataset = dataset.map(format_sample)\n\n# Split: 80% train, 10% validation, 10% test\ntrain_val_split = formatted_dataset['train'].train_test_split(test_size=0.2, seed=42)\nval_test_split = train_val_split['test'].train_test_split(test_size=0.5, seed=42)\n\ntrain_dataset = train_val_split['train']\nval_dataset = val_test_split['train']\ntest_dataset = val_test_split['test']\n\nprint(f\"✅ Dataset split: Train={len(train_dataset)} | Val={len(val_dataset)} | Test={len(test_dataset)}\")\nprint(f\"📊 Enhanced with sample type awareness for multi-format training\")"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrzeraat-tur\u001b[0m (\u001b[33mrzeraat-tur-elyoni\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20251012_232016-mjdrp2k2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rzeraat-tur-elyoni/uk-legal-training/runs/mjdrp2k2' target=\"_blank\">qwen-0.5b-law-r16-seq700-bs5x1-ep2</a></strong> to <a href='https://wandb.ai/rzeraat-tur-elyoni/uk-legal-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rzeraat-tur-elyoni/uk-legal-training' target=\"_blank\">https://wandb.ai/rzeraat-tur-elyoni/uk-legal-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rzeraat-tur-elyoni/uk-legal-training/runs/mjdrp2k2' target=\"_blank\">https://wandb.ai/rzeraat-tur-elyoni/uk-legal-training/runs/mjdrp2k2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ W&B initialized: uk-legal-training/qwen-0.5b-law-r16-seq700-bs5x1-ep2\n",
      "📊 Track at: https://wandb.ai/rzeraat-tur-elyoni/uk-legal-training/runs/mjdrp2k2\n",
      "✅ Training configuration created with validation\n",
      "   Evaluation every 25 steps\n",
      "   Logging every 5 steps\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Training arguments with validation and W&B\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "if WANDB_ENABLED:\n",
    "    try:\n",
    "        wandb.login(key=WANDB_API_KEY, relogin=True)\n",
    "        wandb.init(\n",
    "            project=WANDB_PROJECT,\n",
    "            name=WANDB_RUN_NAME,\n",
    "            mode=\"online\",\n",
    "            config={\n",
    "                \"model\": MODEL_NAME,\n",
    "                \"dataset\": DATASET_NAME,\n",
    "                \"lora_r\": LORA_R,\n",
    "                \"lora_alpha\": LORA_ALPHA,\n",
    "                \"lora_dropout\": LORA_DROPOUT,\n",
    "                \"batch_size\": BATCH_SIZE,\n",
    "                \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS,\n",
    "                \"learning_rate\": LEARNING_RATE,\n",
    "                \"num_epochs\": NUM_EPOCHS,\n",
    "                \"max_seq_length\": MAX_SEQ_LENGTH,\n",
    "                \"effective_batch_size\": BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS,\n",
    "                \"num_gpus\": torch.cuda.device_count(),\n",
    "            }\n",
    "        )\n",
    "        wandb.config.update({\n",
    "            \"gpu_ids\": list(range(torch.cuda.device_count())),\n",
    "            \"gpu_names\": [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())],\n",
    "        })\n",
    "        print(f\"✅ W&B initialized: {WANDB_PROJECT}/{WANDB_RUN_NAME}\")\n",
    "        print(f\"📊 Track at: https://wandb.ai/{wandb.run.entity}/{WANDB_PROJECT}/runs/{wandb.run.id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  W&B initialization failed: {e}\")\n",
    "        WANDB_ENABLED = False\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    fp16=True,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"steps\",\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    max_grad_norm=0.3,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"wandb\" if WANDB_ENABLED else \"none\",\n",
    "    run_name=WANDB_RUN_NAME if WANDB_ENABLED else None,\n",
    "    \n",
    "    # Evaluation settings (note: eval_strategy not evaluation_strategy)\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # SFT-specific\n",
    "    max_length=MAX_SEQ_LENGTH,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=True,\n",
    ")\n",
    "\n",
    "print(\"✅ Training configuration created with validation\")\n",
    "print(f\"   Evaluation every {training_args.eval_steps} steps\")\n",
    "print(f\"   Logging every {training_args.logging_steps} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating trainer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e465e1da324bb79221505aa7362298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/4112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4662c92b8224ca08f7c2a78d7e1828d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/4112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8dcc335053c439d8f4d1cdcfff49c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset:   0%|          | 0/4112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4df8e3d4a1f4ef58da403f5f264d73a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44813f938fa54f85afe04f85328f839f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de3947e950954004a61445935cceb69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing eval dataset:   0%|          | 0/514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Trainer created\n",
      "   Training on 2 GPU(s)\n",
      "   Train: 4112 | Val: 514 | Test: 514\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Create Trainer with validation\n",
    "print(\"Creating trainer...\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"✅ Trainer created\")\n",
    "print(f\"   Training on {torch.cuda.device_count()} GPU(s)\")\n",
    "print(f\"   Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STARTING TRAINING\n",
      "================================================================================\n",
      "Train: 4112 | Val: 514 | Epochs: 2\n",
      "Batch: 5 | Grad Accum: 1\n",
      "Effective Batch: 10\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='538' max='538' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [538/538 1:14:54, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.439400</td>\n",
       "      <td>3.550741</td>\n",
       "      <td>1.849121</td>\n",
       "      <td>172692.000000</td>\n",
       "      <td>0.605122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.436000</td>\n",
       "      <td>3.316838</td>\n",
       "      <td>1.700396</td>\n",
       "      <td>345204.000000</td>\n",
       "      <td>0.623878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>3.329300</td>\n",
       "      <td>3.223716</td>\n",
       "      <td>1.639677</td>\n",
       "      <td>518334.000000</td>\n",
       "      <td>0.631570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.169200</td>\n",
       "      <td>3.152757</td>\n",
       "      <td>1.623133</td>\n",
       "      <td>691613.000000</td>\n",
       "      <td>0.639351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>3.253500</td>\n",
       "      <td>3.095894</td>\n",
       "      <td>1.453412</td>\n",
       "      <td>864503.000000</td>\n",
       "      <td>0.643813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.110500</td>\n",
       "      <td>3.028745</td>\n",
       "      <td>1.529440</td>\n",
       "      <td>1036865.000000</td>\n",
       "      <td>0.649877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>3.041400</td>\n",
       "      <td>2.984836</td>\n",
       "      <td>1.506032</td>\n",
       "      <td>1209256.000000</td>\n",
       "      <td>0.653402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.868900</td>\n",
       "      <td>2.943163</td>\n",
       "      <td>1.481244</td>\n",
       "      <td>1382260.000000</td>\n",
       "      <td>0.658197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>2.814400</td>\n",
       "      <td>2.908906</td>\n",
       "      <td>1.435173</td>\n",
       "      <td>1555075.000000</td>\n",
       "      <td>0.660710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.964600</td>\n",
       "      <td>2.866070</td>\n",
       "      <td>1.455337</td>\n",
       "      <td>1727901.000000</td>\n",
       "      <td>0.664671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>2.250400</td>\n",
       "      <td>2.879651</td>\n",
       "      <td>1.233039</td>\n",
       "      <td>1900293.000000</td>\n",
       "      <td>0.667121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.203600</td>\n",
       "      <td>2.837288</td>\n",
       "      <td>1.243480</td>\n",
       "      <td>2073259.000000</td>\n",
       "      <td>0.670476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>2.209200</td>\n",
       "      <td>2.803102</td>\n",
       "      <td>1.263859</td>\n",
       "      <td>2246736.000000</td>\n",
       "      <td>0.674301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.280800</td>\n",
       "      <td>2.783844</td>\n",
       "      <td>1.222312</td>\n",
       "      <td>2418776.000000</td>\n",
       "      <td>0.675585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>2.431400</td>\n",
       "      <td>2.757694</td>\n",
       "      <td>1.248822</td>\n",
       "      <td>2591426.000000</td>\n",
       "      <td>0.677850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.269200</td>\n",
       "      <td>2.734197</td>\n",
       "      <td>1.221594</td>\n",
       "      <td>2764437.000000</td>\n",
       "      <td>0.681382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>2.291700</td>\n",
       "      <td>2.717901</td>\n",
       "      <td>1.212316</td>\n",
       "      <td>2937849.000000</td>\n",
       "      <td>0.682655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.398700</td>\n",
       "      <td>2.706788</td>\n",
       "      <td>1.219152</td>\n",
       "      <td>3110735.000000</td>\n",
       "      <td>0.683642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>2.325200</td>\n",
       "      <td>2.696623</td>\n",
       "      <td>1.209185</td>\n",
       "      <td>3283609.000000</td>\n",
       "      <td>0.684921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.221400</td>\n",
       "      <td>2.692741</td>\n",
       "      <td>1.204461</td>\n",
       "      <td>3455766.000000</td>\n",
       "      <td>0.685673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>2.370700</td>\n",
       "      <td>2.690805</td>\n",
       "      <td>1.206141</td>\n",
       "      <td>3627680.000000</td>\n",
       "      <td>0.685671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETED\n",
      "================================================================================\n",
      "Train Loss: 4.4536 → 2.3958\n",
      "Val Loss: 2.6908 | Perplexity: 14.74\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/entropy</td><td>█▆▆▆▄▅▄▄▄▄▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/loss</td><td>█▆▅▅▄▄▃▃▃▂▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁▃▃▄▄▅▅▆▆▆▆▇▇▇▇██████</td></tr><tr><td>eval/num_tokens</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>eval/runtime</td><td>▆▃▁▇▆▁▆▇▄▇█▄▅▄▆▄▄▆▄▆▇</td></tr><tr><td>eval/samples_per_second</td><td>▃▆█▂▃█▃▂▅▂▁▅▄▅▃▅▅▃▅▃▂</td></tr><tr><td>eval/steps_per_second</td><td>▃▆█▂▃█▃▂▅▃▁▆▄▅▃▆▆▃▅▃▃</td></tr><tr><td>train/entropy</td><td>█▇▅▅▄▅▄▅▄▄▄▄▄▄▃▃▃▃▃▂▁▂▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▂▂</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▄▆▃▃▅▄▄▃▆▅▃▄█▃▃▃▄▁▄▅▅▇▅▅▃▄▆█▃▄▂▆▄▃▄▃▃▄▃▆</td></tr><tr><td>train/learning_rate</td><td>▂▂▅▅▆▇███████▇▇▇▇▇▆▆▅▅▅▅▅▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▅▅▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▄▂▂▂▂▂▁▁▂▂▁▁▂▁▂▁▂▁▁▁▂</td></tr><tr><td>train/mean_token_accuracy</td><td>▁▂▃▄▃▃▄▄▄▄▄▄▄▄▄▄▄▅▅▆▇█▇▇█▇▇▇█▇▇█▇█▇████▇</td></tr><tr><td>train/num_tokens</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/entropy</td><td>1.20614</td></tr><tr><td>eval/loss</td><td>2.6908</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.68567</td></tr><tr><td>eval/num_tokens</td><td>3627680.0</td></tr><tr><td>eval/runtime</td><td>58.6547</td></tr><tr><td>eval/samples_per_second</td><td>5.711</td></tr><tr><td>eval/steps_per_second</td><td>0.58</td></tr><tr><td>total_flos</td><td>8178347351769600.0</td></tr><tr><td>train/entropy</td><td>1.21562</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>train/global_step</td><td>538</td></tr><tr><td>train/grad_norm</td><td>1.30671</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.3958</td></tr><tr><td>train/mean_token_accuracy</td><td>0.71142</td></tr><tr><td>train/num_tokens</td><td>3696833.0</td></tr><tr><td>train_loss</td><td>2.71826</td></tr><tr><td>train_runtime</td><td>4500.9498</td></tr><tr><td>train_samples_per_second</td><td>1.195</td></tr><tr><td>train_steps_per_second</td><td>0.12</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">qwen-0.5b-law-r16-seq700-bs5x1-ep2</strong> at: <a href='https://wandb.ai/rzeraat-tur-elyoni/uk-legal-training/runs/mjdrp2k2' target=\"_blank\">https://wandb.ai/rzeraat-tur-elyoni/uk-legal-training/runs/mjdrp2k2</a><br> View project at: <a href='https://wandb.ai/rzeraat-tur-elyoni/uk-legal-training' target=\"_blank\">https://wandb.ai/rzeraat-tur-elyoni/uk-legal-training</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251012_232016-mjdrp2k2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ W&B run finished\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Train!\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)} | Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Batch: {BATCH_SIZE} | Grad Accum: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Effective Batch: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS * torch.cuda.device_count()}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Show final metrics\n",
    "train_loss = [x['loss'] for x in trainer.state.log_history if 'loss' in x]\n",
    "eval_loss = [x['eval_loss'] for x in trainer.state.log_history if 'eval_loss' in x]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "if train_loss:\n",
    "    print(f\"Train Loss: {train_loss[0]:.4f} → {train_loss[-1]:.4f}\")\n",
    "if eval_loss:\n",
    "    print(f\"Val Loss: {eval_loss[-1]:.4f} | Perplexity: {math.exp(eval_loss[-1]):.2f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Finish W&B\n",
    "if WANDB_ENABLED:\n",
    "    wandb.finish()\n",
    "    print(\"✅ W&B run finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "✅ Model saved to ./pactoria-v1-simple\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Save model\n",
    "print(\"Saving model...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"✅ Model saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging LoRA weights with base model...\n",
      "✅ Merged model saved to ./qwen-law-merged\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "print(\"Merging LoRA weights with base model...\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "merged_model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "merged_output_dir = \"./qwen-law-merged\"\n",
    "merged_model.save_pretrained(merged_output_dir)\n",
    "tokenizer.save_pretrained(merged_output_dir)\n",
    "print(f\"✅ Merged model saved to {merged_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "HUGGINGFACE_MODEL_NAME = \"rzeraat/pactoria-v1\"\n",
    "\n",
    "print(f\"Pushing to HuggingFace Hub: {HUGGINGFACE_MODEL_NAME}\")\n",
    "\n",
    "if HUGGINGFACE_API_KEY:\n",
    "    login(token=HUGGINGFACE_API_KEY, add_to_git_credential=False)\n",
    "    merged_model.push_to_hub(HUGGINGFACE_MODEL_NAME)\n",
    "    tokenizer.push_to_hub(HUGGINGFACE_MODEL_NAME)\n",
    "    print(f\"✅ Model pushed: https://huggingface.co/{HUGGINGFACE_MODEL_NAME}\")\n",
    "else:\n",
    "    print(\"❌ No HuggingFace API key configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m✅ Gradio installed and imported!\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Install and Import Gradio\n",
    "!pip install -q gradio\n",
    "import gradio as gr\n",
    "print(\"✅ Gradio installed and imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading merged model for inference...\n",
      "✅ Inference model loaded from ./qwen-law-merged\n",
      "   Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Load Merged Model for Inference\n",
    "print(\"Loading merged model for inference...\")\n",
    "\n",
    "# Load the merged model\n",
    "inference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    merged_output_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "inference_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    merged_output_dir,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "inference_tokenizer.pad_token = inference_tokenizer.eos_token\n",
    "\n",
    "print(f\"✅ Inference model loaded from {merged_output_dir}\")\n",
    "print(f\"   Device: {next(inference_model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 16: Create Gradio UI with Streaming Legal Q&A Interface - SAMPLE TYPE AWARE\n\nfrom transformers import TextIteratorStreamer\nfrom threading import Thread\n\ndef generate_legal_answer_stream(\n    question,\n    sample_type=\"case_analysis\",\n    temperature=0.7,\n    max_new_tokens=512,\n    top_p=0.9,\n    repetition_penalty=1.1\n):\n    \"\"\"Generate answer to legal question using fine-tuned model with streaming and sample type awareness\"\"\"\n    \n    # Format prompt matching the training format - include sample type hint\n    type_hints = {\n        'case_analysis': '(IRAC: Issue → Rule → Application → Conclusion)',\n        'educational': '(Teaching: Definition → Legal Basis → Key Elements → Examples)',\n        'client_interaction': '(Client Advice: Understanding → Position → Options → Recommendation)',\n        'statutory_interpretation': '(Statutory: Text → Purpose → Interpretation → Case Law)'\n    }\n    \n    hint = type_hints.get(sample_type, '')\n    \n    prompt = f\"\"\"### Instruction:\n{question}\n\n### Response:\n### Sample Type: {sample_type}\"\"\"\n    \n    # Tokenize\n    inputs = inference_tokenizer(\n        prompt,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=MAX_SEQ_LENGTH\n    ).to(inference_model.device)\n    \n    # Create streamer\n    streamer = TextIteratorStreamer(\n        inference_tokenizer,\n        skip_prompt=True,\n        skip_special_tokens=True\n    )\n    \n    # Generation kwargs\n    generation_kwargs = dict(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        temperature=temperature,\n        top_p=top_p,\n        repetition_penalty=repetition_penalty,\n        do_sample=True,\n        pad_token_id=inference_tokenizer.eos_token_id,\n        eos_token_id=inference_tokenizer.eos_token_id,\n        streamer=streamer,\n    )\n    \n    # Run generation in separate thread\n    thread = Thread(target=inference_model.generate, kwargs=generation_kwargs)\n    thread.start()\n    \n    # Stream the output\n    partial_text = \"\"\n    for new_text in streamer:\n        partial_text += new_text\n        # Extract only the response part (after \"### Response:\")\n        if \"### Response:\" in partial_text:\n            response = partial_text.split(\"### Response:\")[1].strip()\n        else:\n            response = partial_text\n        yield response\n    \n    thread.join()\n\n\n# Sample legal questions for quick testing (with suggested sample types)\nsample_questions = [\n    (\"What are the key duties of company directors under UK law?\", \"educational\"),\n    (\"Explain the concept of consideration in contract law with a relevant case example.\", \"educational\"),\n    (\"What constitutes unfair dismissal in employment law?\", \"client_interaction\"),\n    (\"What is the difference between negligence and breach of statutory duty in tort law?\", \"educational\"),\n    (\"How does the Corporate Manslaughter and Corporate Homicide Act 2007 apply to organizations?\", \"statutory_interpretation\"),\n]\n\n# Create Gradio Interface with Streaming and Sample Type Selection\nwith gr.Blocks(theme=gr.themes.Soft(), title=\"UK Legal AI Assistant - Multi-Format\") as demo:\n    gr.Markdown(\"\"\"\n    # 🏛️ UK Legal AI Assistant (Multi-Format Training)\n    ### Powered by Fine-tuned Qwen2-0.5B with 4 Answer Styles\n    \n    Choose your preferred answer format:\n    - **📋 Case Analysis (IRAC)**: Problem-solving with Issue → Rule → Application → Conclusion\n    - **📚 Educational**: Teaching format with Definition → Legal Basis → Key Elements → Examples\n    - **💼 Client Interaction**: Practical advice with Understanding → Position → Options → Recommendation\n    - **📜 Statutory Interpretation**: Legislative analysis with Text → Purpose → Interpretation → Case Law\n    \n    **Real-time streaming responses!** ✨\n    \"\"\")\n    \n    with gr.Row():\n        with gr.Column(scale=2):\n            question_input = gr.Textbox(\n                label=\"Legal Question\",\n                placeholder=\"Enter your UK law question here...\",\n                lines=3\n            )\n            \n            # Sample Type Selector (NEW!)\n            sample_type_selector = gr.Radio(\n                choices=[\n                    \"case_analysis\",\n                    \"educational\", \n                    \"client_interaction\",\n                    \"statutory_interpretation\"\n                ],\n                value=\"case_analysis\",\n                label=\"📊 Answer Format\",\n                info=\"Select the style of answer you want\"\n            )\n            \n            with gr.Accordion(\"⚙️ Generation Settings\", open=False):\n                temperature = gr.Slider(\n                    minimum=0.1,\n                    maximum=2.0,\n                    value=0.7,\n                    step=0.1,\n                    label=\"Temperature (creativity)\",\n                    info=\"Lower = more focused, Higher = more creative\"\n                )\n                \n                max_tokens = gr.Slider(\n                    minimum=128,\n                    maximum=1024,\n                    value=512,\n                    step=64,\n                    label=\"Max Tokens\",\n                    info=\"Maximum length of generated response\"\n                )\n                \n                top_p = gr.Slider(\n                    minimum=0.1,\n                    maximum=1.0,\n                    value=0.9,\n                    step=0.05,\n                    label=\"Top P (nucleus sampling)\",\n                    info=\"Controls diversity of output\"\n                )\n                \n                repetition_penalty = gr.Slider(\n                    minimum=1.0,\n                    maximum=2.0,\n                    value=1.1,\n                    step=0.1,\n                    label=\"Repetition Penalty\",\n                    info=\"Penalize repeated tokens\"\n                )\n            \n            generate_btn = gr.Button(\"🔍 Generate Answer (Streaming)\", variant=\"primary\", size=\"lg\")\n            \n            gr.Markdown(\"### 📝 Sample Questions\")\n            sample_btns = []\n            for i, (sq, stype) in enumerate(sample_questions):\n                btn = gr.Button(f\"{sq} [{stype}]\", size=\"sm\")\n                sample_btns.append((btn, sq, stype))\n        \n        with gr.Column(scale=3):\n            answer_output = gr.Textbox(\n                label=\"AI Response (Streaming)\",\n                lines=20,\n                show_copy_button=True\n            )\n            \n            gr.Markdown(\"\"\"\n            ---\n            **Model Info:**\n            - Base: Qwen/Qwen2-0.5B\n            - Fine-tuned on: rzeraat/law dataset (multi-format)\n            - LoRA Rank: 32\n            - Training Epochs: 3\n            - Sample Types: 4 (case_analysis, educational, client_interaction, statutory_interpretation)\n            \n            **💡 Multi-format training** - Model adapts answer style based on your selection!\n            \"\"\")\n    \n    # Connect the generate button with streaming + sample type\n    generate_btn.click(\n        fn=generate_legal_answer_stream,\n        inputs=[question_input, sample_type_selector, temperature, max_tokens, top_p, repetition_penalty],\n        outputs=answer_output\n    )\n    \n    # Connect sample question buttons (with auto sample type selection)\n    for btn, question, stype in sample_btns:\n        btn.click(\n            fn=lambda q=question, st=stype: (q, st),\n            outputs=[question_input, sample_type_selector]\n        )\n    \n    # Examples section at bottom\n    gr.Examples(\n        examples=[\n            [\"What are the requirements for a valid contract under English law?\", \"educational\"],\n            [\"A client's company is facing insolvency. What should they do?\", \"client_interaction\"],\n            [\"Explain piercing the corporate veil with case examples.\", \"case_analysis\"],\n            [\"How does Section 172 of the Companies Act 2006 define directors' duties?\", \"statutory_interpretation\"],\n            [\"What is the difference between wrongful and unfair dismissal?\", \"educational\"],\n        ],\n        inputs=[question_input, sample_type_selector],\n        label=\"💡 Example Questions with Sample Types\"\n    )\n\nprint(\"✅ Gradio UI with streaming and sample type selection created!\")\nprint(\"🚀 Launching interface...\")\n\n# Launch the interface\ndemo.launch(\n    share=True,  # Creates public link\n    debug=True,\n    show_error=True\n)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}