{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Packages installed!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install packages\n",
    "!pip install -q transformers datasets peft accelerate trl wandb gradio\n",
    "print(\"‚úÖ Packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-19 11:31:37.139917: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760873497.162622     566 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760873497.169785     566 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gradio installed and imported!\n",
      "================================================================================\n",
      "SYSTEM CHECK\n",
      "================================================================================\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "Number of GPUs: 2\n",
      "  GPU 0: Tesla T4\n",
      "  GPU 1: Tesla T4\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Imports and GPU check\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "import math\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import gradio as gr\n",
    "\n",
    "print(\"‚úÖ Gradio installed and imported!\")\n",
    "print(\"=\"*80)\n",
    "print(\"SYSTEM CHECK\")\n",
    "print(\"=\"*80)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs: {num_gpus}\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API keys loaded from Kaggle Secrets\n",
      "‚úÖ Configuration loaded\n",
      "   Model: Qwen/Qwen2.5-0.5B\n",
      "   Dataset: rzeraat/law\n",
      "   Output: ./pactoria-v1-simple\n",
      "   W&B: legal-training/qwen-0.5b-law-r32-seq1000-bs2x1-ep5\n",
      "   ‚úÖ HuggingFace token configured\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Configuration\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
    "DATASET_NAME = \"rzeraat/law\"\n",
    "OUTPUT_DIR = \"./pactoria-v1-simple\"\n",
    "HUGGINGFACE_MODEL_NAME = \"rzeraat/pactoria-v2\"\n",
    "\n",
    "# API Keys - Load from Kaggle Secrets for security\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    WANDB_API_KEY = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "    HUGGINGFACE_TOKEN = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "    print(\"‚úÖ API keys loaded from Kaggle Secrets\")\n",
    "except Exception as e:\n",
    "    # Fallback to environment variables (for local development)\n",
    "    WANDB_API_KEY = os.getenv('WANDB_API_KEY')\n",
    "    HUGGINGFACE_TOKEN = os.getenv('HUGGINGFACE_TOKEN')\n",
    "    print(f\"‚ö†Ô∏è  Kaggle Secrets not available, using environment variables\")\n",
    "\n",
    "# W&B Configuration\n",
    "WANDB_PROJECT = \"legal-training\"\n",
    "WANDB_ENABLED = True if WANDB_API_KEY else False\n",
    "\n",
    "# LoRA config\n",
    "LORA_R = 32\n",
    "LORA_ALPHA = 64\n",
    "LORA_DROPOUT = 0.05\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "# Training config\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_EPOCHS = 5\n",
    "MAX_SEQ_LENGTH = 1000\n",
    "EVAL_STEPS = 1000\n",
    "\n",
    "\n",
    "\n",
    "# W&B run name\n",
    "WANDB_RUN_NAME = f\"qwen-0.5b-law-r{LORA_R}-seq{MAX_SEQ_LENGTH}-bs{BATCH_SIZE}x{GRADIENT_ACCUMULATION_STEPS}-ep{NUM_EPOCHS}\"\n",
    "\n",
    "print(f\"‚úÖ Configuration loaded\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Dataset: {DATASET_NAME}\")\n",
    "print(f\"   Output: {OUTPUT_DIR}\")\n",
    "if WANDB_ENABLED:\n",
    "    print(f\"   W&B: {WANDB_PROJECT}/{WANDB_RUN_NAME}\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  W&B disabled (WANDB_API_KEY not found)\")\n",
    "if HUGGINGFACE_TOKEN:\n",
    "    print(f\"   ‚úÖ HuggingFace token configured\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  HuggingFace token not found (model push will fail)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "‚úÖ Tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "print(\"‚úÖ Tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "‚úÖ Model loaded (params: 494,032,768)\n",
      "   Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load model - NO DEVICE_MAP!\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"sdpa\",\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "print(f\"‚úÖ Model loaded (params: {model.num_parameters():,})\")\n",
    "print(f\"   Device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA...\n",
      "trainable params: 17,596,416 || all params: 511,629,184 || trainable%: 3.4393\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Apply LoRA\n",
    "print(\"Applying LoRA...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and formatting dataset...\n",
      "üìä Model will learn to identify answer format from question patterns\n",
      "\n",
      "‚úÖ Dataset split:\n",
      "   Train: 8014 samples (80%) - Used for training\n",
      "   Val:   1002 samples (10%) - Used during training for checkpoint selection\n",
      "   Test:  1002 samples (10%) - HELD OUT for final evaluation\n",
      "   Total: 10018 samples\n",
      "\n",
      "Training Examples:\n",
      "  'What are the duties of directors?' ‚Üí Model learns educational format\n",
      "  'My client is facing insolvency...' ‚Üí Model learns client interaction format\n",
      "  'Analyze the case of...' ‚Üí Model learns case analysis format\n",
      "  'Explain Section 172 of the Act' ‚Üí Model learns statutory interpretation format\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Load and format dataset - MODEL LEARNS TO IDENTIFY FORMAT FROM QUESTION\n",
    "def format_sample(sample):\n",
    "    \"\"\"\n",
    "    Format a legal sample for training where the model learns to identify\n",
    "    the appropriate answer format based on the question itself.\n",
    "    \n",
    "    Sample Types (learned implicitly from question patterns):\n",
    "    - case_analysis: Questions about cases, legal problems ‚Üí IRAC methodology\n",
    "    - educational: \"What is...\", \"Explain...\" ‚Üí Teaching format\n",
    "    - client_interaction: Client scenarios, practical advice ‚Üí Practical guidance\n",
    "    - statutory_interpretation: Questions about statutes/acts ‚Üí Legislative analysis\n",
    "    \n",
    "    The model learns which format to use by seeing patterns in questions paired\n",
    "    with their corresponding answer structures during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build instruction (question only - NO sample type hint!)\n",
    "    instruction = f\"\"\"### Instruction:\n",
    "{sample['question']}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    \n",
    "    # Start response with metadata (NO sample type - model must infer it!)\n",
    "    response = f\"\"\"\n",
    "### Topic: {sample.get('topic', 'Corporate Law')}\n",
    "### Difficulty: {sample.get('difficulty', 'intermediate')}\n",
    "\"\"\"\n",
    "    \n",
    "    # Add reasoning section (chain of thought)\n",
    "    if 'reasoning' in sample and sample['reasoning']:\n",
    "        response += f\"\"\"\n",
    "### Reasoning:\n",
    "{sample['reasoning']}\n",
    "\"\"\"\n",
    "    \n",
    "    # Add answer WITHOUT type hints\n",
    "    # The model learns the structure from the answer itself paired with question patterns\n",
    "    response += f\"\"\"\n",
    "### Answer:\n",
    "{sample['answer']}\"\"\"\n",
    "    \n",
    "    # Add case citations if available\n",
    "    if 'case_citation' in sample and sample['case_citation']:\n",
    "        response += f\"\"\"\n",
    "\n",
    "### Case Citation:\n",
    "{sample['case_citation']}\"\"\"\n",
    "    \n",
    "    return {\"text\": instruction + response}\n",
    "\n",
    "print(\"Loading and formatting dataset...\")\n",
    "print(\"üìä Model will learn to identify answer format from question patterns\")\n",
    "print()\n",
    "dataset = load_dataset(DATASET_NAME)\n",
    "formatted_dataset = dataset.map(format_sample)\n",
    "\n",
    "# Split: 80% train, 10% validation, 10% test\n",
    "# Train: Used for training the model\n",
    "# Val: Used during training to monitor overfitting and select best checkpoint\n",
    "# Test: HELD OUT - used only for final evaluation after training completes\n",
    "train_val_split = formatted_dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "val_test_split = train_val_split['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "train_dataset = train_val_split['train']\n",
    "val_dataset = val_test_split['train']\n",
    "test_dataset = val_test_split['test']\n",
    "\n",
    "print(f\"‚úÖ Dataset split:\")\n",
    "print(f\"   Train: {len(train_dataset)} samples (80%) - Used for training\")\n",
    "print(f\"   Val:   {len(val_dataset)} samples (10%) - Used during training for checkpoint selection\")\n",
    "print(f\"   Test:  {len(test_dataset)} samples (10%) - HELD OUT for final evaluation\")\n",
    "print(f\"   Total: {len(formatted_dataset['train'])} samples\")\n",
    "print()\n",
    "print(\"Training Examples:\")\n",
    "print(\"  'What are the duties of directors?' ‚Üí Model learns educational format\")\n",
    "print(\"  'My client is facing insolvency...' ‚Üí Model learns client interaction format\")  \n",
    "print(\"  'Analyze the case of...' ‚Üí Model learns case analysis format\")\n",
    "print(\"  'Explain Section 172 of the Act' ‚Üí Model learns statutory interpretation format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrzeraat-tur\u001b[0m (\u001b[33mrzeraat-tur-elyoni\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20251019_113157-qona7x89</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rzeraat-tur-elyoni/legal-training/runs/qona7x89' target=\"_blank\">qwen-0.5b-law-r32-seq1000-bs2x1-ep5</a></strong> to <a href='https://wandb.ai/rzeraat-tur-elyoni/legal-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rzeraat-tur-elyoni/legal-training' target=\"_blank\">https://wandb.ai/rzeraat-tur-elyoni/legal-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rzeraat-tur-elyoni/legal-training/runs/qona7x89' target=\"_blank\">https://wandb.ai/rzeraat-tur-elyoni/legal-training/runs/qona7x89</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ W&B initialized: legal-training/qwen-0.5b-law-r32-seq1000-bs2x1-ep5\n",
      "üìä Track at: https://wandb.ai/rzeraat-tur-elyoni/legal-training/runs/qona7x89\n",
      "‚úÖ Training configuration created with validation\n",
      "   Evaluation every 1000 steps\n",
      "   Logging every 3 steps\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Training arguments with validation and W&B\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "if WANDB_ENABLED:\n",
    "    try:\n",
    "        wandb.login(key=WANDB_API_KEY, relogin=True)\n",
    "        wandb.init(\n",
    "            project=WANDB_PROJECT,\n",
    "            name=WANDB_RUN_NAME,\n",
    "            mode=\"online\",\n",
    "            config={\n",
    "                \"model\": MODEL_NAME,\n",
    "                \"dataset\": DATASET_NAME,\n",
    "                \"lora_r\": LORA_R,\n",
    "                \"lora_alpha\": LORA_ALPHA,\n",
    "                \"lora_dropout\": LORA_DROPOUT,\n",
    "                \"batch_size\": BATCH_SIZE,\n",
    "                \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS,\n",
    "                \"learning_rate\": LEARNING_RATE,\n",
    "                \"num_epochs\": NUM_EPOCHS,\n",
    "                \"max_seq_length\": MAX_SEQ_LENGTH,\n",
    "                \"effective_batch_size\": BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS,\n",
    "                \"num_gpus\": torch.cuda.device_count(),\n",
    "            }\n",
    "        )\n",
    "        wandb.config.update({\n",
    "            \"gpu_ids\": list(range(torch.cuda.device_count())),\n",
    "            \"gpu_names\": [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())],\n",
    "        })\n",
    "        print(f\"‚úÖ W&B initialized: {WANDB_PROJECT}/{WANDB_RUN_NAME}\")\n",
    "        print(f\"üìä Track at: https://wandb.ai/{wandb.run.entity}/{WANDB_PROJECT}/runs/{wandb.run.id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  W&B initialization failed: {e}\")\n",
    "        WANDB_ENABLED = False\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    fp16=True,\n",
    "    logging_steps=3,\n",
    "    save_strategy=\"steps\",\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    max_grad_norm=0.3,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"wandb\" if WANDB_ENABLED else \"none\",\n",
    "    run_name=WANDB_RUN_NAME if WANDB_ENABLED else None,\n",
    "    \n",
    "    # Evaluation settings (note: eval_strategy not evaluation_strategy)\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    save_total_limit=1,\n",
    "    \n",
    "    # SFT-specific\n",
    "    max_length=MAX_SEQ_LENGTH,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=True,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training configuration created with validation\")\n",
    "print(f\"   Evaluation every {training_args.eval_steps} steps\")\n",
    "print(f\"   Logging every {training_args.logging_steps} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating trainer...\n",
      "‚úÖ Trainer created\n",
      "   Training on 2 GPU(s)\n",
      "   Train: 8014 samples | Val: 1002 samples\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Create Trainer with validation\n",
    "print(\"Creating trainer...\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer created\")\n",
    "print(f\"   Training on {torch.cuda.device_count()} GPU(s)\")\n",
    "print(f\"   Train: {len(train_dataset)} samples | Val: {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STARTING TRAINING\n",
      "================================================================================\n",
      "Train: 8014 samples | Val: 1002 samples\n",
      "Epochs: 5\n",
      "Batch per GPU: 2 | Grad Accum: 1\n",
      "Effective Batch: 4\n",
      "GPUs: 2\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='515' max='7230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 515/7230 23:13 < 5:04:03, 0.37 it/s, Epoch 0.36/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 10: Train!\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Train: {len(train_dataset)} samples | Val: {len(val_dataset)} samples\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Batch per GPU: {BATCH_SIZE} | Grad Accum: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Effective Batch: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS * torch.cuda.device_count()}\")\n",
    "print(f\"GPUs: {torch.cuda.device_count()}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Show final metrics\n",
    "train_loss = [x['loss'] for x in trainer.state.log_history if 'loss' in x]\n",
    "eval_loss = [x['eval_loss'] for x in trainer.state.log_history if 'eval_loss' in x]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "if train_loss:\n",
    "    print(f\"Train Loss: {train_loss[0]:.4f} ‚Üí {train_loss[-1]:.4f}\")\n",
    "if eval_loss:\n",
    "    print(f\"Val Loss: {eval_loss[-1]:.4f} | Perplexity: {math.exp(eval_loss[-1]):.2f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Finish W&B\n",
    "if WANDB_ENABLED:\n",
    "    wandb.finish()\n",
    "    print(\"‚úÖ W&B run finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Save model\n",
    "print(\"Saving model...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"‚úÖ Model saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "print(\"Merging LoRA weights with base model...\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "merged_model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "merged_output_dir = \"./qwen-law-merged\"\n",
    "merged_model.save_pretrained(merged_output_dir)\n",
    "tokenizer.save_pretrained(merged_output_dir)\n",
    "print(f\"‚úÖ Merged model saved to {merged_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-test-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11b: Evaluate on Test Set (HELD OUT - Never Seen During Training)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"Evaluating model on held-out test set...\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "test_loss = test_results.get('eval_loss', 0)\n",
    "test_perplexity = math.exp(test_loss) if test_loss > 0 else 0\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Perplexity: {test_perplexity:.2f}\")\n",
    "print()\n",
    "print(\"Comparison with Validation Set:\")\n",
    "if eval_loss:\n",
    "    val_loss_final = eval_loss[-1]\n",
    "    val_perplexity = math.exp(val_loss_final)\n",
    "    print(f\"  Val Loss:  {val_loss_final:.4f} | Perplexity: {val_perplexity:.2f}\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f} | Perplexity: {test_perplexity:.2f}\")\n",
    "    print()\n",
    "    if abs(test_loss - val_loss_final) < 0.1:\n",
    "        print(\"‚úÖ Test and validation losses are similar - good generalization!\")\n",
    "    elif test_loss > val_loss_final + 0.2:\n",
    "        print(\"‚ö†Ô∏è  Test loss is higher than validation - possible overfitting\")\n",
    "    else:\n",
    "        print(\"‚úÖ Test performance looks good!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "print(f\"Pushing to HuggingFace Hub: {HUGGINGFACE_MODEL_NAME}\")\n",
    "\n",
    "if HUGGINGFACE_TOKEN:\n",
    "    try:\n",
    "        login(token=HUGGINGFACE_TOKEN, add_to_git_credential=False)\n",
    "        merged_model.push_to_hub(HUGGINGFACE_MODEL_NAME)\n",
    "        tokenizer.push_to_hub(HUGGINGFACE_MODEL_NAME)\n",
    "        print(f\"‚úÖ Model pushed: https://huggingface.co/{HUGGINGFACE_MODEL_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to push to HuggingFace Hub: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå No HuggingFace token configured\")\n",
    "    print(\"   Set HUGGINGFACE_TOKEN in Kaggle Secrets to enable model push\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Load Merged Model for Inference\n",
    "print(\"Loading merged model for inference...\")\n",
    "\n",
    "# Load the merged model\n",
    "inference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    merged_output_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "inference_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    merged_output_dir,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "inference_tokenizer.pad_token = inference_tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úÖ Inference model loaded from {merged_output_dir}\")\n",
    "print(f\"   Device: {next(inference_model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Create Gradio UI with Streaming - MODEL AUTO-DETECTS FORMAT\n",
    "\n",
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "def generate_legal_answer_stream(\n",
    "    question,\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=512,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate answer to legal question using fine-tuned model with streaming.\n",
    "    \n",
    "    The model automatically identifies the appropriate answer format based on\n",
    "    the question pattern (no manual format selection needed).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simple prompt - model decides format based on question\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "{question}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = inference_tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH\n",
    "    ).to(inference_model.device)\n",
    "    \n",
    "    # Create streamer\n",
    "    streamer = TextIteratorStreamer(\n",
    "        inference_tokenizer,\n",
    "        skip_prompt=True,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # Generation kwargs\n",
    "    generation_kwargs = dict(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        do_sample=True,\n",
    "        pad_token_id=inference_tokenizer.eos_token_id,\n",
    "        eos_token_id=inference_tokenizer.eos_token_id,\n",
    "        streamer=streamer,\n",
    "    )\n",
    "    \n",
    "    # Run generation in separate thread\n",
    "    thread = Thread(target=inference_model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "    \n",
    "    # Stream the output\n",
    "    partial_text = \"\"\n",
    "    for new_text in streamer:\n",
    "        partial_text += new_text\n",
    "        yield partial_text\n",
    "    \n",
    "    thread.join()\n",
    "\n",
    "\n",
    "# Sample legal questions demonstrating different formats the model learned\n",
    "sample_questions = [\n",
    "    \"What are the key duties of company directors under UK law?\",  # Educational\n",
    "    \"My client's company is facing insolvency. What should they do?\",  # Client interaction\n",
    "    \"Analyze the case of Salomon v Salomon & Co Ltd and its implications.\",  # Case analysis\n",
    "    \"Explain how Section 172 of the Companies Act 2006 defines directors' duties.\",  # Statutory\n",
    "    \"What is the difference between negligence and breach of statutory duty?\",  # Educational\n",
    "    \"A director wants to enter into a contract with their company. What are the legal requirements?\",  # Client interaction\n",
    "]\n",
    "\n",
    "# Create Gradio Interface - NO SAMPLE TYPE SELECTOR\n",
    "with gr.Blocks(theme=gr.themes.Soft(), title=\"UK Legal AI Assistant - Pactoria v1\") as demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # üèõÔ∏è UK Legal AI Assistant - Pactoria v1\n",
    "    ### Powered by Fine-tuned Qwen2-0.5B\n",
    "    \n",
    "    **ü§ñ Intelligent Format Detection**\n",
    "    \n",
    "    The model automatically adapts its answer style based on your question:\n",
    "    - **Educational questions** (\"What is...\", \"Explain...\") ‚Üí Teaching format with definitions and examples\n",
    "    - **Client scenarios** (\"My client...\", \"What should...\") ‚Üí Practical advice and recommendations\n",
    "    - **Case analysis** (\"Analyze the case...\", legal problems) ‚Üí IRAC methodology\n",
    "    - **Statutory questions** (\"Section X says...\", \"The Act provides...\") ‚Üí Legislative interpretation\n",
    "    \n",
    "    **No need to select a format - the model knows!** ‚ú®\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            question_input = gr.Textbox(\n",
    "                label=\"Legal Question\",\n",
    "                placeholder=\"Ask any UK law question - the model will adapt its answer format automatically...\",\n",
    "                lines=4\n",
    "            )\n",
    "            \n",
    "            with gr.Accordion(\"‚öôÔ∏è Generation Settings\", open=False):\n",
    "                temperature = gr.Slider(\n",
    "                    minimum=0.1,\n",
    "                    maximum=2.0,\n",
    "                    value=0.7,\n",
    "                    step=0.1,\n",
    "                    label=\"Temperature (creativity)\",\n",
    "                    info=\"Lower = more focused, Higher = more creative\"\n",
    "                )\n",
    "                \n",
    "                max_tokens = gr.Slider(\n",
    "                    minimum=128,\n",
    "                    maximum=1024,\n",
    "                    value=512,\n",
    "                    step=64,\n",
    "                    label=\"Max Tokens\",\n",
    "                    info=\"Maximum length of generated response\"\n",
    "                )\n",
    "                \n",
    "                top_p = gr.Slider(\n",
    "                    minimum=0.1,\n",
    "                    maximum=1.0,\n",
    "                    value=0.9,\n",
    "                    step=0.05,\n",
    "                    label=\"Top P (nucleus sampling)\",\n",
    "                    info=\"Controls diversity of output\"\n",
    "                )\n",
    "                \n",
    "                repetition_penalty = gr.Slider(\n",
    "                    minimum=1.0,\n",
    "                    maximum=2.0,\n",
    "                    value=1.1,\n",
    "                    step=0.1,\n",
    "                    label=\"Repetition Penalty\",\n",
    "                    info=\"Penalize repeated tokens\"\n",
    "                )\n",
    "            \n",
    "            generate_btn = gr.Button(\"üîç Generate Answer (Streaming)\", variant=\"primary\", size=\"lg\")\n",
    "            \n",
    "            gr.Markdown(\"### üìù Sample Questions\")\n",
    "            gr.Markdown(\"*Click any question to try it - notice how the model adapts its format!*\")\n",
    "            sample_btns = []\n",
    "            for sq in sample_questions:\n",
    "                btn = gr.Button(sq, size=\"sm\")\n",
    "                sample_btns.append((btn, sq))\n",
    "        \n",
    "        with gr.Column(scale=3):\n",
    "            answer_output = gr.Textbox(\n",
    "                label=\"AI Response (Streaming)\",\n",
    "                lines=22,\n",
    "                show_copy_button=True\n",
    "            )\n",
    "            \n",
    "            gr.Markdown(\"\"\"\n",
    "            ---\n",
    "            **Model Info:**\n",
    "            - **Name**: Pactoria v1 (`rzeraat/pactoria-v1`)\n",
    "            - **Base**: Qwen/Qwen2-0.5B (494M params)\n",
    "            - **Dataset**: rzeraat/law (10,018 samples)\n",
    "            - **LoRA**: Rank 32, Alpha 64\n",
    "            - **Training**: Multi-GPU (2√ó T4)\n",
    "            \n",
    "            **üéØ Intelligent Features:**\n",
    "            - Automatic format detection from question patterns\n",
    "            - Multi-format training (4 answer styles learned)\n",
    "            - Step-by-step legal reasoning\n",
    "            - Real case citations and statutory references\n",
    "            \n",
    "            **üí° The model learned different answer structures from question patterns!**\n",
    "            \"\"\")\n",
    "    \n",
    "    # Connect the generate button\n",
    "    generate_btn.click(\n",
    "        fn=generate_legal_answer_stream,\n",
    "        inputs=[question_input, temperature, max_tokens, top_p, repetition_penalty],\n",
    "        outputs=answer_output\n",
    "    )\n",
    "    \n",
    "    # Connect sample question buttons\n",
    "    for btn, question in sample_btns:\n",
    "        btn.click(\n",
    "            fn=lambda q=question: q,\n",
    "            outputs=question_input\n",
    "        )\n",
    "    \n",
    "    # Examples section\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            [\"What are the requirements for a valid contract under English law?\"],\n",
    "            [\"My client's company is facing insolvency. What legal options are available?\"],\n",
    "            [\"Analyze the doctrine of piercing the corporate veil with relevant case examples.\"],\n",
    "            [\"How does Section 172 of the Companies Act 2006 define directors' duties?\"],\n",
    "            [\"What is the difference between wrongful dismissal and unfair dismissal?\"],\n",
    "            [\"A director wants to use company property for personal use. Is this allowed?\"],\n",
    "        ],\n",
    "        inputs=[question_input],\n",
    "        label=\"üí° Example Questions (Different Formats)\"\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Gradio UI created!\")\n",
    "print(\"ü§ñ Model will automatically detect and use the appropriate answer format\")\n",
    "print(\"üöÄ Launching interface...\")\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch(\n",
    "    share=True,  # Creates public link\n",
    "    debug=True,\n",
    "    show_error=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
